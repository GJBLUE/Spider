爬取http://www.17ce.com/，该网站用AJAX将数据返回。
利用fiddler4进行抓包进行查看，发现数据分两块进行了存储。
思路：最终效果是要输入一个网址，就能得到它返回的特定数据。但要分析之后，发现需要先将需要查询的网址POST过去，发现一个返回值，填入第二个POST请求中，这才可以获得需要的一系列数据。
web_ce.py: 为urllib+urllib2
web_spider.py: 为requests


Q: POST请求后，出现{"message":"非法请求"}
S: 初步猜测爬虫被发现，添加User-Agent和Referer等一系列属性,还是被BAN。排除HTTP和URL问题，应该还是爬虫被发现了。恩，只保留User-Agent和Referer之后OK，估计某个属性会改变或者我打错了。

Q: 在多添加了几个headers属性后，出现httplib.BadStatusLine: ''
S: 添加Host无效,报错的开始是打开req的问题。将POST内容全部放入也没用。尝试加入代理，error，cookie似乎无用。是httplib遇到了无法识别的状态码。尝试try..catch...强制抛出错误，success。

Q: page = response.read()读不出内容。
S: 好吧，response就没有获取到。将headers里面所有属性都加进去，error，此处应该是第一个问题的锅。将代码重构一遍，解决。

想了下，既然我需要数据在json里，那直接获取josn得了。但数据并不是json。
Q: 网页返回的数据是str，但这个str是一层层字典嵌套的格式，不是JSON，无法直接json.loads进行处理。
S: 使用json.dumps()先将数据转化为json字符串。

Q: 遇到了最大问题，先dumps,再loads之后，出来的格式是unicode,然后根据官方文档，dumps时出来的是str，str在loads之后，就是unicode。但是，在ipython中尝试，它最后居然是dict。。。。dict。。
S: json.loads().encode('utf-8')结果将unicode变为str，问题应该是是从网页出来的是str类型，所以最后变为unicode，ipython测试时一开始给的就是dict，所以最后也是dict。但我需要的是dict，将网页一开始就变为字典：
data = eval(page)
datas = json.dumps(data,ensure_ascii=False)
jsondatas = json.loads(datas)
但eval可能会有风险，这个之后在考虑新的写法。写法改为eval(name, {}, {})提高了安全性。好吧，eval弃用，原来问题在于python中json模块不支持数字当主键。采用re替换吧。发现了真正的问题在于，JSON字符串中，不能包含单引号，而必须是双引号。恍然大悟，由于数据中存在value没有""，放弃re。
import ast
data = ast.literal_eval(page)
解决了eval的安全性问题。

Q: POST过去之后，请求到的数据不全。
S: 请求的POST表单中，存在重复的属性，在进过urllib.urlencode()之后，这些重复的属性会变成一个。直接从fiddler上把post串拿了下来，这次得到大量数据了。

Q: 在爬取过程中Fidler开着可取的大量数据，但关闭Fiddler之后，只有几个甚至没有数据....将请求的报文数据放在Fiddler中模拟请求时，居然，居然。。。。取到了所有数据。。。
A: 查阅了不少资料，坑估计是urllib2的，因为请求报文里面Connection: close，urllib2不能建立长链接，尝试去换下requests。已换requests，但是在不开fiddler的情况下还是连了之后很快就断了，不是connection的问题。问题出在了网页后台会间断的发送数据过来，而不是一次发送过来。for循环可以处理，但你不知道要循环几次。
